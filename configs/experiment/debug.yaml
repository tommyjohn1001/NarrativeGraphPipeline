# @package _global_

defaults:
  - override /logger: null

task: debug
seed: 307
batch_size: 5
num_workers: 8

trainer:
  _target_: pytorch_lightning.Trainer

  gpus: 1
  min_epochs: 1
  max_epochs: 1
  resume_from_checkpoint: ${PATH.checkpoint}

datamodule:
  batch_size: ${batch_size}
  num_workers: ${num_workers}
